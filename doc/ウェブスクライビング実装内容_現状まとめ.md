# ウェブスクライビング機能 現状実装まとめ（inventory_scraper）

## 対象範囲
- 対象コード: `inventory_scraper/` 配下のPython実装
- 連携先: Googleスプレッドシート / GAS Webアプリ
- 備考: Googleフォーム経由のアップロード機能も実装は存在するが、現行の `main.py` では未使用

## 全体概要
ローカルPCでSeleniumを用いてブラウザ操作を行い、GoogleスプレッドシートのCSVをダウンロード → 各URLのスクレイピング → 結果CSV作成 → **GAS WebアプリへPOST送信でスプレッドシート更新**、という流れで動作する。

**注意**: Googleフォーム経由のアップロード機能（`upload_to_google_form`）は実装されているが、現行の`main.py`では使用されていない。代わりにGAS Webアプリへの直接POST送信（`update_spreadsheet_via_gas`）を使用している。

## 実行フロー（`main.py`）
1. ブラウザ初期化（Chromeプロファイル利用・User-Agent上書き）
2. スプレッドシートから在庫管理シートCSVをダウンロード
3. `仕入れ元URL` 列のURLに対してスクレイピング実行
4. 結果をCSVに保存（`data/upload_data.csv`）
5. GAS WebアプリにCSVをPOST送信して直接更新

## 設定・環境変数（`src/config.py`）
- `SPREADSHEET_ID`, `SHEET_GID`: 在庫管理シートの取得先
- `SUPPLIER_SHEET_GID`: 仕入れ元マスターシート（空なら自動検出）
- `GAS_WEB_APP_URL`: GAS Webアプリのエンドポイント（必須）
- `GOOGLE_FORM_URL`: Googleフォーム（アップロード経由時に使用）
- `CHROME_PROFILE_PATH`, `CHROME_PROFILE_NAME`: ログイン済みChromeプロファイル
- `CHROME_USER_AGENT`: User-Agent上書き（未設定ならデフォルト）
- `DOWNLOAD_FOLDER`: ダウンロード先（未設定時は既定Downloads）
- `LOG_LEVEL`, `ENABLE_DEBUG_MODE`: ログ/デバッグ制御
- `DATA_DIR`, `LOGS_DIR`: データ/ログ保存先（自動作成）

## ブラウザ初期化（`src/browser.py`）
- ChromeDriverは `webdriver-manager` で自動管理
- 既存Chromeプロファイルを指定してGoogleログインを維持
- Bot検知回避向けのオプションとUser-Agent上書き
- ダウンロード先を `data/` に固定

## スプレッドシートCSV取得（`src/downloader.py`）
- `https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=csv&gid={SHEET_GID}`
  をブラウザで開いてCSVを取得
- ダウンロード完了判定: 新規CSVの存在とサイズ安定で判定
- CSV読み込み後に `仕入れ元URL` 列が空の行は除外

## スクレイピング構成（`src/scraper.py`, `src/configurable_scraper.py`）
### スクレイパー選択方針
優先順位:
1. 設定ファイル/スプレッドシート設定ベースの `ConfigurableScraper`
2. 既存のハードコーディングスクレイパー（Amazon / メルカリ / Yahoo!）

### 設定ベーススクレイパー（`ConfigurableScraper`）
- `price_selectors`, `stock_selectors`, `stock_keywords` を設定で定義
- Yahoo!オークションは `__NEXT_DATA__` から価格抽出を優先
- 価格が取れない場合はフォールバックで広範囲探索
- 404判定時は `売り切れ` / `仕入れ価格=0` を返す
- `ENABLE_DEBUG_MODE` 有効時は価格候補のデバッグ出力

### 既存スクレイパー（`AmazonScraper`, `MercariScraper`, `YahooScraper`）
- 価格と在庫判定をサイト別セレクタで取得
- 例外時は404判定で `売り切れ` or `不明` にフォールバック

### スクレイピング結果の共通カラム
- `仕入れ元URL`
- `仕入れ価格`
- `在庫ステータス`（在庫あり / 売り切れ / 不明）
- `最終更新日時`

## セレクタ設定の読み込み（`src/spreadsheet_config_loader.py`）
- 仕入れ元マスターシートをCSVとして取得
- 期待ヘッダー: `サイト名`, `URLパターン（カンマ区切り）`, `価格セレクタ（カンマ区切り）` など
- `有効フラグ` が `有効` の行のみ使用
- スプレッドシート設定がJSON設定より優先
- `SUPPLIER_SHEET_GID` 未指定の場合は GID 0-20 を探索

## JSON設定（`config/scraper_config.json`）
- 主要サイト（楽天市場 / Amazon / メルカリ / Yahoo!オークション / Yahoo!ショッピング）を定義
- サイト別セレクタとキーワードの既定値を保持
- スプレッドシート設定で上書き可能

## 結果保存と更新
### CSV保存（`src/uploader.py`）
- `data/upload_data.csv` に UTF-8 BOM付きで保存
- 現状 `main.py` ではアップロード処理は実行せず、保存のみ使用

### GAS Webアプリ更新（`src/spreadsheet_updater.py`）
- CSV内容をJSONでPOST送信して更新
- 50KB超のCSVはチャンク分割送信
- レスポンスで更新件数や未一致件数をログ出力
- GAS Webアプリ側で `doPost` 実装・デプロイが必要

### Googleフォーム経由アップロード（`src/uploader.py`）
- フォームのファイルアップロードUIを自動操作
- 多数のセレクタでファイル入力/送信ボタンを探索
- 送信完了判定は画面テキスト/タイトル/URLで確認
- 現行フローでは未使用（必要時は `upload_to_google_form` を呼び出す）

## エラー/例外ハンドリング
- CSVダウンロード失敗時はリトライ（`retry_count=1`）
- スクレイピング失敗時:
  - 404判定できれば `仕入れ価格=0`, `売り切れ`
  - その他は `仕入れ価格=-1`, `在庫ステータス=不明`
- GAS更新時:
  - HTMLエラーページ検出時は明示的なエラー
  - JSONレスポンス解析エラーは警告を出して失敗扱い

## ログ
- `logs/scraper_YYYYMMDD_HHMMSS.log` に出力
- `LOG_LEVEL` で制御（DEBUGで詳細ログ）

## 主要ファイル一覧
- `inventory_scraper/main.py`: エントリーポイント
- `inventory_scraper/src/browser.py`: Selenium初期化
- `inventory_scraper/src/downloader.py`: CSVダウンロード
- `inventory_scraper/src/scraper.py`: 既存スクレイパー/制御
- `inventory_scraper/src/configurable_scraper.py`: 設定ベーススクレイパー
- `inventory_scraper/src/spreadsheet_config_loader.py`: スプレッドシート設定読込
- `inventory_scraper/src/spreadsheet_updater.py`: GAS Webアプリ更新
- `inventory_scraper/src/uploader.py`: CSV保存/フォームアップロード
- `inventory_scraper/config/scraper_config.json`: サイト別セレクタ定義
