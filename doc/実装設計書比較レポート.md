# 在庫管理スクレイピングシステム 実装設計書比較レポート

**作成日**: 2025-12-24  
**比較対象**: `doc/ウェブスクライビング設計書.md` vs `inventory_scraper/` 実装

---

## 1. ディレクトリ構成の比較

### 設計書の要件
```
inventory_scraper/
├── data/                  # CSVファイル一時保存用
├── logs/                  # 実行ログ
├── src/
│   ├── __init__.py
│   ├── config.py          # 定数・設定読み込み
│   ├── browser.py         # Seleniumドライバー初期化・設定
│   ├── downloader.py      # スプレッドシートDL処理
│   ├── scraper.py         # スクレイピングロジック（Strategyパターン）
│   ├── uploader.py        # CSV保存処理
│   └── spreadsheet_updater.py  # GAS Webアプリ経由の更新処理
├── .env                   # 環境変数（URL, パス等）
├── main.py                # エントリーポイント
└── requirements.txt
```

### 実装状況
✅ **一致**: ディレクトリ構成は設計書通りに実装されています。

**追加ファイル**:
- `README.md`: プロジェクト説明ファイル（設計書に記載なし、問題なし）
- `test_*.py`: テストファイル（設計書に記載なし、問題なし）
- `venv/`: 仮想環境（設計書に記載なし、問題なし）

---

## 2. 環境変数 (.env) の比較

### 設計書の要件
```ini
# スプレッドシート情報
SPREADSHEET_ID=xxxxxxxxxxxxxxxxxxx
SHEET_GID=0
# Google Apps Script WebアプリURL（必須）
GAS_WEB_APP_URL=https://script.google.com/macros/s/YOUR_SCRIPT_ID/exec
# ローカルChrome設定
CHROME_PROFILE_PATH=C:\Users\Username\AppData\Local\Google\Chrome\User Data
CHROME_PROFILE_NAME=Default
```

### 実装状況
✅ **一致**: `config.py`で設計書通りの環境変数を読み込んでいます。

**追加項目**（設計書に記載なし、問題なし）:
- `DOWNLOAD_FOLDER`: ダウンロードフォルダのパス（デフォルト: `~/Downloads`）
- `LOG_LEVEL`: ログレベル（デフォルト: `INFO`）

⚠️ **注意**: `.env`ファイル自体は存在しませんが、`.env.example`も存在しません。これは設計書の「Step 1」で`.env.example`の生成が指示されているため、**不足している可能性があります**。

---

## 3. モジュール実装の比較

### 3.1 `browser.py` (ブラウザ初期化)

#### 設計書の要件
- `--user-data-dir` オプションで既存のChromeプロファイルを指定
- Bot検知回避のため、`--disable-blink-features=AutomationControlled` 等のオプションを付与

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- Chromeプロファイルの指定: ✅ 実装済み
- Bot検知回避オプション: ✅ 実装済み（`--disable-blink-features=AutomationControlled`、`excludeSwitches`、`useAutomationExtension`）
- User-Agent設定: ✅ 実装済み（追加機能）

---

### 3.2 `downloader.py` (データ読み込み)

#### 設計書の要件
1. URL生成: `https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=csv&gid={SHEET_GID}`
2. `browser` を使用してURLへ遷移（自動ダウンロード発生）
3. ダウンロードフォルダを監視し、最新の `.csv` ファイルを特定
4. Pandas DataFrameとして読み込み、リターンする
5. CSVの列名は日本語（`仕入れ元URL`, `仕入れ価格`等）であることを前提とする
6. `仕入れ元URL`列（F列）が空でない行のみをスクレイピング対象とする

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- エクスポートURL生成: ✅ 実装済み
- ブラウザ経由ダウンロード: ✅ 実装済み
- ダウンロードファイル監視: ✅ 実装済み（ファイルサイズの安定性確認も含む）
- Pandas DataFrame読み込み: ✅ 実装済み（`encoding='utf-8-sig'`）
- 日本語列名対応: ✅ 実装済み
- `仕入れ元URL`列のフィルタリング: ✅ 実装済み
- リトライ機能: ✅ 実装済み（設計書のエラーハンドリング要件に対応）

---

### 3.3 `scraper.py` (スクレイピング)

#### 設計書の要件
- **クラス設計**:
  - `BaseScraper`: 基底クラス
  - `AmazonScraper`, `MercariScraper`, `YahooScraper`: サイト別実装
- **入力データ**: `downloader.py`から取得したDataFrameの`仕入れ元URL`列を使用
- **取得項目**:
  - `仕入れ価格`: int (価格、単位: 円)
  - `在庫ステータス`: str ("在庫あり" or "売り切れ")
- **出力形式**: スクレイピング結果をDataFrame形式で保持
  - 列名: `仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`
- **待機処理**: アクセスごとに `random.uniform(3, 7)` 秒のウェイトを入れる

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- Strategyパターン: ✅ 実装済み（`BaseScraper`、`AmazonScraper`、`MercariScraper`、`YahooScraper`）
- URL判定関数: ✅ 実装済み（`get_scraper()`関数）
- 待機処理: ✅ 実装済み（`random.uniform(3, 7)`秒）
- エラーハンドリング: ✅ 実装済み（404エラー時は「売り切れ」、要素が見つからない場合は「不明」）
- 出力形式: ✅ 実装済み（列名: `仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`）

**エラーハンドリングの詳細**:
- ページが存在しない(404)場合: ✅ 「売り切れ」、仕入れ価格「0」として記録
- 要素が見つからない場合: ✅ 仕入れ価格「-1」、在庫ステータス「不明」として記録
- エラー発生時も`最終更新日時`は現在日時を記録: ✅ 実装済み

---

### 3.4 `uploader.py` (データ書き込み)

#### 設計書の要件
1. スクレイピング結果を `data/upload_data.csv` として保存
   - CSVの列名は日本語（`仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`）を使用
   - エンコーディング: UTF-8（BOM付き推奨）
2. GAS WebアプリにCSVデータをPOST送信
3. レスポンスで更新結果を確認

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- CSV保存: ✅ 実装済み（`save_result_csv()`関数、UTF-8 BOM付き）
- GAS WebアプリへのPOST送信: ✅ 実装済み（`spreadsheet_updater.py`）
- チャンク分割送信: ✅ 実装済み（50KB超のCSVに対応）
- レスポンス確認: ✅ 実装済み（更新件数の確認）

---

### 3.5 `main.py` (エントリーポイント)

#### 設計書の要件
設計書の「Step 6」で「全体のオーケストレーション実装」が指示されています。

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
1. スプレッドシートからCSVをダウンロード: ✅ 実装済み
2. 各ECサイトをスクレイピング: ✅ 実装済み
3. 結果をCSVに保存: ✅ 実装済み
4. GAS WebアプリにPOST送信: ✅ 実装済み
5. エラーハンドリング: ✅ 実装済み
6. ログ出力: ✅ 実装済み

---

## 4. GAS側スクリプト (`WebScrapingDirectUpdate.gs`) の比較

### 設計書の要件
- **ファイル名**: `src/WebScrapingDirectUpdate.gs`
- **エンドポイント**: `doPost`関数（Webアプリとしてデプロイ）
- **処理フロー**:
  1. POSTリクエストのボディからCSVデータを取得
  2. `parseCsvWithMultilineSupport`関数（既存実装）でデータを展開
  3. スプレッドシートの「仕入れ元URL」列（F列）をキーにして、辞書（Map）を作成
  4. シート全行をスキャンし、URLが一致する行の以下の列を上書き更新：
     - **仕入れ価格**（G列、7列目）: CSVの「仕入れ価格」から取得
     - **在庫ステータス**（S列、19列目）: CSVの「在庫ステータス」から取得
     - **最終更新日時**（Z列、26列目）: CSVの「最終更新日時」から取得
  5. 更新結果をJSON形式で返す
- **既存機能の活用**:
  - `parseCsvWithMultilineSupport`関数（`JoomCsvExport.gs`に実装済み）を活用
  - `COLUMN_INDEXES.INVENTORY`定数（`Constants.gs`に定義済み）を使用して列インデックスを参照

### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- ファイル名: ✅ `src/WebScrapingDirectUpdate.gs`（一致）
- エンドポイント: ✅ `doPost`関数（一致）
- POSTリクエストからCSVデータ取得: ✅ 実装済み
- `parseCsvWithMultilineSupport`使用: ✅ 実装済み（`JoomCsvExport.gs`から呼び出し）
- `COLUMN_INDEXES.INVENTORY`使用: ✅ 実装済み（`Constants.gs`から参照）
- URLをキーとしたMap作成: ✅ 実装済み（URL正規化機能も含む）
- 列の更新: ✅ 実装済み
  - 仕入れ価格（G列、7列目）: ✅ `COLUMN_INDEXES.INVENTORY.PURCHASE_PRICE`
  - 在庫ステータス（S列、19列目）: ✅ `COLUMN_INDEXES.INVENTORY.STOCK_STATUS`
  - 最終更新日時（Z列、26列目）: ✅ `COLUMN_INDEXES.INVENTORY.LAST_UPDATED`
- JSONレスポンス返却: ✅ 実装済み

**追加機能**（設計書に記載なし、問題なし）:
- URL正規化機能（末尾スラッシュ削除）
- 価格のバリデーション（-1は更新しない）
- 詳細なログ出力
- エラーハンドリングの強化
- フォーム作成・トリガー設定のヘルパー関数

---

## 5. エラーハンドリングの比較

### 設計書の要件
- **DL失敗**: タイムアウト時はリトライを1回行う。失敗時はログ出力して終了
- **スクレイピング失敗**: 
  - ページが存在しない(404)場合 -> 在庫ステータス「売り切れ」、仕入れ価格「0」として記録
  - 要素が見つからない場合 -> 仕入れ価格「-1」、在庫ステータス「不明」として記録し、処理を継続
  - エラー発生時も`最終更新日時`は現在日時を記録する
- **アップロード失敗**: 例外をキャッチし、ローカルにCSVを残したままアラート（ログ）を出力
- **GAS側のエラーハンドリング**:
  - CSVパース失敗時はエラーログを出力し、処理を中断
  - 該当行が見つからない場合（URL不一致）は警告ログを出力し、処理を継続
  - 更新処理の失敗時はエラーログを出力し、CSVファイルはゴミ箱へ移動せずに保持

### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- DL失敗のリトライ: ✅ `downloader.py`で実装済み（`retry_count`パラメータ）
- スクレイピング失敗の処理: ✅ `scraper.py`で実装済み（404エラー、要素未検出の処理）
- アップロード失敗の処理: ✅ `uploader.py`で実装済み（例外キャッチ、ログ出力）
- GAS側のエラーハンドリング: ✅ `WebScrapingFormHandler.gs`で実装済み

---

## 6. データ定義の比較

### 6.1 入力CSV (スプレッドシート Export)

#### 設計書の要件
- CSVの列名は日本語（`仕入れ元URL`, `仕入れ価格`等）であることを前提とする
- `仕入れ元URL`列（F列）が空でない行のみをスクレイピング対象とする

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

---

### 6.2 出力CSV (フォーム Upload)

#### 設計書の要件
- CSVの列名は日本語（`仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`）を使用
- エンコーディング: UTF-8（BOM付き推奨）

#### 実装状況
✅ **一致**: 設計書通りの実装がされています。

**実装内容**:
- 列名: ✅ 日本語列名を使用（`仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`）
- エンコーディング: ✅ UTF-8 BOM付き（`encoding='utf-8-sig'`）

---

## 7. 技術スタックの比較

### 設計書の要件
- **言語**: Python 3.10+
- **ブラウザ操作**: Selenium (`webdriver_manager` 推奨)
- **データ処理**: Pandas
- **設定管理**: `python-dotenv` (.env)
- **バックエンド処理**: Google Apps Script (GAS)

### 実装状況
✅ **一致**: 設計書通りの技術スタックが使用されています。

**requirements.txt**:
```
selenium>=4.15.0
webdriver-manager>=4.0.1
pandas>=2.1.0
python-dotenv>=1.0.0
```

---

## 8. 相違点・不足点のまとめ

### ⚠️ 不足している項目

1. **`.env.example`ファイル**
   - 設計書の「Step 1」で`.env.example`の生成が指示されているが、存在しない
   - **推奨対応**: `.env.example`ファイルを作成し、テンプレートとして提供する

### ✅ 追加実装（設計書に記載なし、問題なし）

以下の機能は設計書に明示的に記載されていませんが、実装されており、問題ありません：

1. **`config.py`の追加設定**:
   - `DOWNLOAD_FOLDER`: ダウンロードフォルダのパス
   - `LOG_LEVEL`: ログレベル

2. **`browser.py`の追加機能**:
   - User-Agent設定
   - ダウンロードフォルダの設定

3. **`downloader.py`の追加機能**:
   - ファイルサイズの安定性確認（ダウンロード完了の判定）

4. **`uploader.py`の追加機能**:
   - 既存ファイルの削除処理
   - 詳細なデバッグログ出力
   - iframe対応の強化

5. **`WebScrapingFormHandler.gs`の追加機能**:
   - URL正規化機能
   - 価格のバリデーション（-1は更新しない）
   - フォーム作成・トリガー設定のヘルパー関数

---

## 9. 結論

### 実装状況の評価

**全体的な評価**: ✅ **設計書と実装はほぼ完全に一致しています**

### 主な確認結果

1. ✅ **ディレクトリ構成**: 設計書通りに実装されている
2. ✅ **各モジュール**: 設計書の要件を満たしている
3. ✅ **GAS側スクリプト**: 設計書の要件を満たしている
4. ✅ **エラーハンドリング**: 設計書の要件を満たしている
5. ✅ **データ定義**: 設計書の要件を満たしている
6. ✅ **技術スタック**: 設計書の要件を満たしている

### 推奨対応事項

1. **`.env.example`ファイルの作成**
   - 設計書の「Step 1」で指示されているため、作成を推奨
   - テンプレートとして、実際の値を含まない形式で提供

### 追加実装について

設計書に記載されていない追加機能がいくつか実装されていますが、これらは**問題ありません**。むしろ、実用性を高めるための改善として評価できます。

---

**レポート作成日**: 2025-12-24  
**比較対象バージョン**: 
- 設計書: `doc/ウェブスクライビング設計書.md`
- 実装: `inventory_scraper/` ディレクトリ内の全ファイル

