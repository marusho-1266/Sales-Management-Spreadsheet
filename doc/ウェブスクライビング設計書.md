AIエディタ（Cursor, Windsurf, GitHub Copilotなど）に読み込ませて、即座に実装コードを生成させるための**詳細設計仕様書**を作成しました。

このマークダウンテキストをコピーして、AIエディタのチャット欄やコンテキストとして貼り付けてください。

---

# 在庫管理スクレイピングシステム 設計仕様書

## 1. プロジェクト概要
本システムは、ローカルPC上のPythonとGoogleスプレッドシートを連携させた在庫管理ツールである。
Google Cloud Platform (GCP) やGASのウェブアプリ公開機能を利用せず、ローカルブラウザの自動操作（Selenium）のみでデータの読み書きを完結させる「ハイブリッド・ローカル実行モデル」を採用する。

## 2. 制約条件と前提（厳守）
1.  **GCP利用不可**: Google Cloud ConsoleでのAPI有効化、サービスアカウントキーの発行は行わない。
2.  **GASウェブアプリ化不可**: `doPost` 等を用いたWeb API公開は行わない。
3.  **ユーザー操作**: スプレッドシート利用者はITリテラシーが低いため、ツール操作を行わない。シートの閲覧・編集のみを行う。
4.  **実行環境**: 管理者のローカルPC（Windows/Mac）にて、タスクスケジューラ等でPythonを実行する。
5.  **認証方式**: Seleniumにて、ローカルChromeの**ユーザープロファイル（ログイン済み状態）**を流用する。

## 3. システムアーキテクチャ

### 3.1 データフロー
1.  **READ**: Python (Selenium) がスプレッドシートの「隠しエクスポートURL」にアクセスし、CSVをダウンロード。
2.  **PROCESS**: Pythonが各ECサイト（Amazon, Mercari, Yahoo!等）をスクレイピングし、価格・在庫情報を取得。
3.  **WRITE**: Pythonが結果CSVをGAS WebアプリにPOST送信。
4.  **UPDATE**: GAS Webアプリの`doPost`関数がCSV内容をスプレッドシートに反映。

### 3.2 技術スタック
*   **言語**: Python 3.10+
*   **ブラウザ操作**: Selenium (`webdriver_manager` 推奨)
*   **データ処理**: Pandas
*   **設定管理**: `python-dotenv` (.env)
*   **バックエンド処理**: Google Apps Script (GAS)

---

## 4. ディレクトリ構成
```text
inventory_scraper/
├── data/                  # CSVファイル一時保存用
├── logs/                  # 実行ログ
├── src/
│   ├── __init__.py
│   ├── config.py          # 定数・設定読み込み
│   ├── browser.py         # Seleniumドライバー初期化・設定
│   ├── downloader.py      # スプレッドシートDL処理
│   ├── scraper.py         # スクレイピングロジック（Strategyパターン）
│   ├── uploader.py        # CSV保存処理
│   └── spreadsheet_updater.py  # GAS Webアプリ経由の更新処理
├── .env                   # 環境変数（URL, パス等）
├── main.py                # エントリーポイント
└── requirements.txt
```

**注意**: GAS側の実装は `src/WebScrapingFormHandler.gs` として既存のGASプロジェクトに追加する。

---

## 5. 詳細機能設計

### 5.1 環境変数 (.env)
以下の機密情報を管理する。
```ini
# スプレッドシート情報
SPREADSHEET_ID=xxxxxxxxxxxxxxxxxxx
SHEET_GID=0
# Google Apps Script WebアプリURL（必須）
GAS_WEB_APP_URL=https://script.google.com/macros/s/YOUR_SCRIPT_ID/exec
# ローカルChrome設定
CHROME_PROFILE_PATH=C:\Users\Username\AppData\Local\Google\Chrome\User Data
CHROME_PROFILE_NAME=Default
```

### 5.2 機能モジュール詳細

#### A. `browser.py` (ブラウザ初期化)
*   **機能**: Selenium WebDriverのインスタンスを生成する。
*   **仕様**:
    *   `--user-data-dir` オプションで既存のChromeプロファイルを指定（Googleログイン状態の維持）。
    *   Bot検知回避のため、`--disable-blink-features=AutomationControlled` 等のオプションを付与。

#### B. `downloader.py` (データ読み込み)
*   **機能**: スプレッドシートの「在庫管理」シートをCSVとして取得する。
*   **ロジック**:
    1.  URL生成: `https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=csv&gid={SHEET_GID}`
       *   `SHEET_GID`は「在庫管理」シートのGIDを指定（通常は`0`）
    2.  `browser` を使用してURLへ遷移（自動ダウンロード発生）。
    3.  ダウンロードフォルダを監視し、最新の `.csv` ファイルを特定。
    4.  Pandas DataFrameとして読み込み、リターンする。
    5.  **重要**: CSVの列名は日本語（`仕入れ元URL`, `仕入れ価格`等）であることを前提とする。
    6.  `仕入れ元URL`列（F列）が空でない行のみをスクレイピング対象とする。

#### C. `scraper.py` (スクレイピング)
*   **機能**: URLリストに基づきデータを取得する。
*   **クラス設計**:
    *   `BaseScraper`: 基底クラス。
    *   `AmazonScraper`, `MercariScraper`, `YahooScraper`: サイト別実装。
*   **入力データ**:
    *   `downloader.py`から取得したDataFrameの`仕入れ元URL`列を使用
*   **取得項目**:
    *   `仕入れ価格`: int (価格、単位: 円)
    *   `在庫ステータス`: str ("在庫あり" or "売り切れ")
*   **出力形式**:
    *   スクレイピング結果をDataFrame形式で保持
    *   列名: `仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`
*   **待機処理**: アクセスごとに `random.uniform(3, 7)` 秒のウェイトを入れる。

#### D. `uploader.py` (CSV保存)
*   **機能**: スクレイピング結果をCSVファイルとして保存する。
*   **ロジック**:
    1.  スクレイピング結果を `data/upload_data.csv` として保存。
       *   CSVの列名は日本語（`仕入れ元URL`, `仕入れ価格`, `在庫ステータス`, `最終更新日時`）を使用
       *   エンコーディング: UTF-8（BOM付き推奨）

#### E. `spreadsheet_updater.py` (データ書き込み)
*   **機能**: 結果CSVをGAS Webアプリ経由でスプレッドシートに反映する。
*   **ロジック**:
    1.  保存されたCSVファイルを読み込む。
    2.  CSV内容をJSON形式でGAS WebアプリにPOST送信。
    3.  50KB超のCSVの場合はチャンク分割して送信。
    4.  レスポンスで更新件数を確認。

### 5.3 GAS側スクリプト仕様 (`WebScrapingDirectUpdate.gs`)
*   **ファイル名**: `src/WebScrapingDirectUpdate.gs`
*   **エンドポイント**: `doPost`関数（Webアプリとしてデプロイ）
*   **処理フロー**:
    1.  POSTリクエストのボディからCSVデータを取得。
    2.  `parseCsvWithMultilineSupport`関数（既存実装）でデータを展開。
    3.  スプレッドシートの「仕入れ元URL」列（F列）をキーにして、辞書（Map）を作成。
    4.  シート全行をスキャンし、URLが一致する行の以下の列を上書き更新：
        *   **仕入れ価格**（G列、7列目）: CSVの「仕入れ価格」から取得
        *   **在庫ステータス**（S列、19列目）: CSVの「在庫ステータス」から取得
        *   **最終更新日時**（Z列、26列目）: CSVの「最終更新日時」から取得
    5.  更新結果をJSON形式で返す。
*   **既存機能の活用**:
    *   `parseCsvWithMultilineSupport`関数（`JoomCsvExport.gs`に実装済み）を活用
    *   `COLUMN_INDEXES.INVENTORY`定数（`Constants.gs`に定義済み）を使用して列インデックスを参照

---

## 6. データ定義

### 6.1 入力CSV (スプレッドシート Export)
スプレッドシートの「在庫管理」シートからエクスポートされるCSV形式。

| カラム名（日本語） | 列インデックス | 説明 | 必須 |
| :--- | :--- | :--- | :--- |
| `商品ID` | A列（1列目） | 管理用ID | 任意 |
| `商品名` | B列（2列目） | 商品名称 | 任意 |
| `SKU` | C列（3列目） | 商品管理コード | 任意 |
| `ASIN` | D列（4列目） | Amazon商品コード | 任意 |
| `仕入れ元` | E列（5列目） | 仕入れ先サイト名 | 任意 |
| `仕入れ元URL` | F列（6列目） | スクレイピング対象URL | **必須** |
| `仕入れ価格` | G列（7列目） | 現在の仕入れ価格 | 任意 |
| `在庫ステータス` | S列（19列目） | 現在の在庫ステータス | 任意 |
| `最終更新日時` | Z列（26列目） | 最終更新日時 | 任意 |

**注意**: Python側では、`仕入れ元URL`列（F列）をキーとしてスクレイピング対象を特定する。

### 6.2 出力CSV (GAS Webアプリ POST)
GAS Webアプリ経由で送信するCSV形式。**日本語列名を使用**する。

| カラム名（日本語） | 説明 | データ型 | 必須 |
| :--- | :--- | :--- | :--- |
| `仕入れ元URL` | スクレイピング対象URL（キー項目） | 文字列 | **必須** |
| `仕入れ価格` | 取得した価格（数値） | 整数 | **必須** |
| `在庫ステータス` | "在庫あり" or "売り切れ" | 文字列 | **必須** |
| `最終更新日時` | YYYY-MM-DD HH:MM:SS形式 | 文字列 | **必須** |

**列名マッピング**:
*   Python側で生成するCSVの列名は、スプレッドシートの列名と一致させる（日本語列名を使用）
*   GAS側では、`仕入れ元URL`をキーとして在庫管理シートの該当行を検索し、以下の列を更新：
    *   `仕入れ価格`（G列、7列目）← CSVの`仕入れ価格`
    *   `在庫ステータス`（S列、19列目）← CSVの`在庫ステータス`
    *   `最終更新日時`（Z列、26列目）← CSVの`最終更新日時`

---

## 7. エラーハンドリング
*   **DL失敗**: タイムアウト時はリトライを1回行う。失敗時はログ出力して終了。
*   **スクレイピング失敗**: 
    *   ページが存在しない(404)場合 -> 在庫ステータス「売り切れ」、仕入れ価格「0」として記録。
    *   要素が見つからない場合 -> 仕入れ価格「-1」、在庫ステータス「不明」として記録し、処理を継続。
    *   エラー発生時も`最終更新日時`は現在日時を記録する。
*   **更新失敗**: 例外をキャッチし、ローカルにCSVを残したままアラート（ログ）を出力。
*   **GAS側のエラーハンドリング**:
    *   CSVパース失敗時はエラーログを出力し、処理を中断。
    *   該当行が見つからない場合（URL不一致）は警告ログを出力し、処理を継続。
    *   更新処理の失敗時はエラーログを出力し、エラーレスポンスを返す。

---

## 8. 実装ステップ（AIへの指示用）

1.  **Step 1**: `requirements.txt` とディレクトリ構造の作成。
2.  **Step 2**: `src/browser.py` の実装（プロファイル読み込み機能付き）。
3.  **Step 3**: `src/downloader.py` の実装（エクスポートURL経由）。
4.  **Step 4**: `src/scraper.py` の実装（Amazon/Mercariのセレクタ仮実装含む）。
5.  **Step 5**: `src/uploader.py` の実装（CSV保存）。
6.  **Step 6**: `src/spreadsheet_updater.py` の実装（GAS Webアプリ経由の更新）。
7.  **Step 7**: `main.py` で全体のオーケストレーション実装。
8.  **Step 8**: GASコード (`WebScrapingDirectUpdate.gs`) の実装。
   *   既存の`parseCsvWithMultilineSupport`関数を活用
   *   `COLUMN_INDEXES.INVENTORY`定数を使用して列インデックスを参照
   *   `onFormSubmit`トリガーを設定

---

**AIエディタへの指示:**
この仕様書に基づき、まずはプロジェクトのディレクトリ構造と `requirements.txt`、および `.env.example` を生成してください。その後、各モジュールの実装コードを順次作成してください。